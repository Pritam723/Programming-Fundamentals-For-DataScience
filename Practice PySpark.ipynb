{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb61e72-89e2-414f-8cab-2a1c92800ddc",
   "metadata": {},
   "source": [
    "<h3>Part-1: Installation, Creating Session, Reading Data, DataFrames</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ddbd4-3a50-473d-bdb1-f245702261a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69352c01-ddac-4c0b-b2b5-2e9c6a22dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef32849-8fa5-4b39-901d-8b129c4cfaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PractiseSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15841462880>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always start a Spark Session.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PractiseSpark').getOrCreate()\n",
    "# For the first time it will take time. But when we use the same appName\n",
    "# later, it will be fetched quite quickly.\n",
    "spark\n",
    "\n",
    "# In local there will be 1 cluster only. In a cloud we can create multiple\n",
    "# clusters and instances.\n",
    "# See here local[*] is present in the Master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0412d2a1-3c5f-4c46-895e-ba7ef125a217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: string, _c1: string]\n",
      "+-----+---+\n",
      "|  _c0|_c1|\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "|  Ram| 21|\n",
      "|Shyam| 22|\n",
      "|Sunny| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('TestData.csv')\n",
    "print(df_pyspark) # See here since we are reading CSV so it is adding a \n",
    "# layer of extra row. But we want name and age to be our column names.\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b217a8ff-263d-4d13-9395-81bb5d007f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Name: string, Age: string]\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|  Ram| 21|\n",
      "|Shyam| 22|\n",
      "|Sunny| 32|\n",
      "+-----+---+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# So, a better way here is:\n",
    "\n",
    "df_pyspark = spark.read.option('header', 'true').csv('TestData.csv')\n",
    "print(df_pyspark) # See the column names are correct now. \n",
    "df_pyspark.show()\n",
    "# So, now it shows the desired data.\n",
    "print(type(df_pyspark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91068eb6-e6e9-48ab-8b99-d93c25b79cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "type(pd.read_csv('TestData.csv'))\n",
    "\n",
    "# <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "# vs\n",
    "# pandas.core.frame.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fc77a58-9423-492f-9ca5-0bb24400af05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Back to PySpark.\n",
    "# Check column information.\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043242e-5680-4884-8fe3-37b2d09a7ab5",
   "metadata": {},
   "source": [
    "<h3>Part-2: Reading Data Column Wise. Adding/Dropping/Renaming Columns</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9667962f-fdfb-49be-9968-a2ce1148f1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PractiseSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15841462880>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PractiseSpark').getOrCreate()\n",
    "spark\n",
    "# See this time it will be fast. As the session is alreeady created.\n",
    "# Since we are running in local, so it is the only one master node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74d4919a-4746-4c40-8626-d25f3f3f93c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Name: string, age: string, Experience: string, Salary: string]\n",
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.option('header','true').csv('TestData2.csv')\n",
    "print(df_spark)\n",
    "df_spark.show()\n",
    "df_spark.printSchema()\n",
    "# But see all columns are considered as String.\n",
    "# To make PySpark infer the schema, we can use inferSchema = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ef2d442-acca-4746-b361-cfdb27e6e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Name: string, age: int, Experience: int, Salary: int]\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.option('header','true').csv('TestData2.csv', inferSchema = True)\n",
    "print(df_spark)\n",
    "# df_spark.show()\n",
    "df_spark.printSchema()\n",
    "# See now the columns are considered as they are supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4851b91-a0f7-4254-907a-3da9e7d6f98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way to read csv:\n",
    "df_spark = spark.read.csv('TestData2.csv', header = True, inferSchema=True)\n",
    "df_spark.show()\n",
    "# So this also works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "789b2cbc-4698-4dc8-a977-971b5ec9a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'age', 'Experience', 'Salary']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', age=31, Experience=10, Salary=30000),\n",
       " Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000),\n",
       " Row(Name='Sunny', age=29, Experience=4, Salary=20000)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_spark.columns)\n",
    "df_spark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02256000-80d0-4d6c-87a4-b1d701b0fd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|     Paul|\n",
      "|   Harsha|\n",
      "|  Shubham|\n",
      "|   Mahesh|\n",
      "|     NULL|\n",
      "|     NULL|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show only the names.\n",
    "df_spark.select('Name').show()\n",
    "\n",
    "# df_spark['Name'] Does not work here.\n",
    "\n",
    "type(df_spark.select('Name')) # Is also a DataFrame.\n",
    "\n",
    "df_spark.select(['Name', 'Age']) # Can apply .show() here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15504d1b-6ae2-4aa2-80fa-348cc6184126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]\n",
      "DataFrame[summary: string, Name: string, age: string, Experience: string, Salary: string]\n"
     ]
    }
   ],
   "source": [
    "print(df_spark.dtypes)\n",
    "print(df_spark.describe()) # So, it is a DF again. So, show() can be applied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "767f51a8-29af-4f62-bac7-d3fdd40a226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+------------------+-----------------+\n",
      "|summary|  Name|               age|        Experience|           Salary|\n",
      "+-------+------+------------------+------------------+-----------------+\n",
      "|  count|     7|                 8|                 7|                8|\n",
      "|   mean|  NULL|              28.5| 5.428571428571429|          25750.0|\n",
      "| stddev|  NULL|5.3718844791323335|3.8234863173611093|9361.776388210581|\n",
      "|    min|Harsha|                21|                 1|            15000|\n",
      "|    max| Sunny|                36|                10|            40000|\n",
      "+-------+------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6d8f351b-9a66-4eae-9560-32502f145f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+------------------------+\n",
      "|     Name| age|Experience|Salary|Expreience After 2 Years|\n",
      "+---------+----+----------+------+------------------------+\n",
      "|    Krish|  31|        10| 30000|                      12|\n",
      "|Sudhanshu|  30|         8| 25000|                      10|\n",
      "|    Sunny|  29|         4| 20000|                       6|\n",
      "|     Paul|  24|         3| 20000|                       5|\n",
      "|   Harsha|  21|         1| 15000|                       3|\n",
      "|  Shubham|  23|         2| 18000|                       4|\n",
      "|   Mahesh|NULL|      NULL| 40000|                    NULL|\n",
      "|     NULL|  34|        10| 38000|                      12|\n",
      "|     NULL|  36|      NULL|  NULL|                    NULL|\n",
      "+---------+----+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding & Dropping Columns\n",
    "df_spark.withColumn('Expreience After 2 Years', df_spark['Experience'] + 2).show()\n",
    "# Not an inplace operation, so df_spark is not changed.\n",
    "\n",
    "df_new = df_spark.withColumn('Expreience After 2 Years', df_spark['Experience'] + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a6aa1ec-8d4d-4510-8735-058887266306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping columns\n",
    "df_new = df_new.drop('Expreience After 2 Years')\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "080daec8-f3bf-4361-8562-a1dec4213b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+------+\n",
      "|     Name| age| YOE|Salary|\n",
      "+---------+----+----+------+\n",
      "|    Krish|  31|  10| 30000|\n",
      "|Sudhanshu|  30|   8| 25000|\n",
      "|    Sunny|  29|   4| 20000|\n",
      "|     Paul|  24|   3| 20000|\n",
      "|   Harsha|  21|   1| 15000|\n",
      "|  Shubham|  23|   2| 18000|\n",
      "|   Mahesh|NULL|NULL| 40000|\n",
      "|     NULL|  34|  10| 38000|\n",
      "|     NULL|  36|NULL|  NULL|\n",
      "+---------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming Columns.\n",
    "df_spark.withColumnRenamed('Experience', 'YOE').show()\n",
    "# This is also not inplace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84207a34-386c-4e39-a479-ad096b5862a0",
   "metadata": {},
   "source": [
    "<h3>Part-3: Handling Missing Values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b636974-9b40-4ca3-8860-a644f2dffb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can drop columns, rows, or may replace using mean/median/mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fddedb-c12e-4312-81b8-4bc2c118344c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PractiseSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x210c96b9e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PractiseSpark').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffffdcc-7304-4fa1-a4a7-d01095e6e4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('TestData2.csv', header=True, inferSchema=True)\n",
    "df.show() # See this data has NULL values too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0fc2e4-d40e-4cab-9064-d36ffb711a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These are the available options to handle NULLs. df has a property na.\n",
    "# df.na.drop()\n",
    "# df.na.fill()\n",
    "# df.na.replace()\n",
    "\n",
    "# Deleting All rows having atleast one NULL value in Column.\n",
    "\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ff9109-a39f-4654-8266-00d93dd27fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how = 'any', thresh = 2, subset=['Age','Experience']).show()\n",
    "\n",
    "# how: Checks if 'any' value is NULL or 'all' values are NULL.\n",
    "# thresh: If the row doesn't have atleast 2 Non NULL values, it will be deleted.\n",
    "# subset: Only checks 'Age' & 'Experience' column for NULL. If NULL, delete entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26effc5-a924-43ef-8a84-04769c5e4444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|         Name| age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|        Krish|  31|        10| 30000|\n",
      "|    Sudhanshu|  30|         8| 25000|\n",
      "|        Sunny|  29|         4| 20000|\n",
      "|         Paul|  24|         3| 20000|\n",
      "|       Harsha|  21|         1| 15000|\n",
      "|      Shubham|  23|         2| 18000|\n",
      "|       Mahesh|NULL|      NULL| 40000|\n",
      "|Missing Value|  34|        10| 38000|\n",
      "|Missing Value|  36|      NULL|  NULL|\n",
      "+-------------+----+----------+------+\n",
      "\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh|  0|         0| 40000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "|     NULL| 36|         0|  NULL|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling the Missing Values\n",
    "\n",
    "df.na.fill('Missing Value').show() # Won't work on age or experience\n",
    "# as those are integers.\n",
    "\n",
    "df.na.fill(0, ['Age', 'Experience']).show()\n",
    "# Will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c320bcce-e579-46e6-8e2f-af9fe21e034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling missing data with mean/median etc...\n",
    "# We will use Imputer function. (In sklearn also it is there)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a09e1f-16e6-4a7c-be00-c6ae4be3956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|         28|                 5|         40000|\n",
      "|     NULL|  34|        10| 38000|         34|                10|         38000|\n",
      "|     NULL|  36|      NULL|  NULL|         36|                 5|         25750|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Impute: assign (a value) to something by inference from the value\n",
    "# of the products or processes to which it contributes.\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols = ['age','Experience','Salary'],\n",
    "    outputCols = ['age_imputed','Experience_imputed','Salary_imputed'] \n",
    ").setStrategy(\"mean\")\n",
    "\n",
    "# Now I will use fit and transform.\n",
    "imputer.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4edd3-012b-4f15-849e-ae71d680718b",
   "metadata": {},
   "source": [
    "<h3>Part-4: Filter Operations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a84bb4e7-1f2e-4a15-a647-4a80a96a2c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PractiseSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x210c96b9e50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PractiseSpark').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edfc2ad2-c5c3-4135-a2b5-9109cc1fdfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('TestData3.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c901ae50-02d5-4c3a-9fc5-574ba888d27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "| Harsha| 21|\n",
      "|Shubham| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1. People having salary < 20000\n",
    "df.filter('Salary < 20000').select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c94cff0-7a66-4bcb-a343-6d5011e59ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|Harsha| 21|         1| 15000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another Syntax. In Pandas we used to write .loc[]\n",
    "df.filter((df['Salary'] < 20000) & ~(df['Age'] > 22)).show()\n",
    "# ~ is not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d4c3ce-cee3-4b97-9f09-dbc49f8940ac",
   "metadata": {},
   "source": [
    "<h3>Part-5: Group By & Aggregate Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76e68015-0e5d-4842-a284-39d66498b6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PractiseSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x210c96b9e50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PractiseSpark').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e660f776-82d2-410e-b864-1bd2632d2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('TestData4.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebd05297-8122-4ed6-8e52-947068bd29ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check who is having max salary.\n",
    "# help(df.groupBy('Name').sum)\n",
    "df.groupBy('Name').sum('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a655797a-9660-4251-a58a-2c8952f73d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check avg salary of each department.\n",
    "df.groupBy('Departments').mean('salary').show()\n",
    "# Using .count() we can get no. of people in the department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3112d493-6769-4145-bf97-6404d6a49d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+\n",
      "| Departments|sum(Salary)|count(name)|\n",
      "+------------+-----------+-----------+\n",
      "|         IOT|      15000|          2|\n",
      "|    Big Data|      15000|          4|\n",
      "|Data Science|      43000|          4|\n",
      "+------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Departments').agg({'Salary': 'sum', 'name' : 'count'}).show()\n",
    "# If we do not use groupBy the agg is applied on whole df."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a29dd7-691b-4add-a90e-b49aae994a31",
   "metadata": {},
   "source": [
    "<h3>Part-6: Spark MLlib</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30346c2e-be0c-4986-aae0-19b9ee998d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PractiseSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x210c96b9e50>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With respect to Spark ML there are 2 different techniques.\n",
    "# One is RDD technique and other one is the DataFrame APIs (More popular now).\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PractiseSpark').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0324c7fa-b135-4f41-b430-030b06b55e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('TestData3.csv', header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d49291a9-2384-4611-8c24-70042c95e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Question1. Based on Age and Experience predict Salary.\n",
    "# Basic question, not much data preprocessing, transformation or \n",
    "# standardization is required here.\n",
    "# Later we will see that it is a Linear Regression Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93e71c64-6853-48f2-add8-4f5d5941d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is bit different from sklearn. In sklearn we do\n",
    "# train, test split first. (We divide into dependent/ independent features etc).\n",
    "# In PySpark we first must group the independent features. We call\n",
    "# it a VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46c6b6fd-8119-46e8-87ad-b1d68f084c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+--------------------+\n",
      "|     Name|age|Experience|Salary|Independent Features|\n",
      "+---------+---+----------+------+--------------------+\n",
      "|    Krish| 31|        10| 30000|         [31.0,10.0]|\n",
      "|Sudhanshu| 30|         8| 25000|          [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|          [29.0,4.0]|\n",
      "|     Paul| 24|         3| 20000|          [24.0,3.0]|\n",
      "|   Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
      "|  Shubham| 23|         2| 18000|          [23.0,2.0]|\n",
      "+---------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureAssembler = VectorAssembler(inputCols = ['age', 'Experience'], outputCol=\"Independent Features\")\n",
    "trainingData = featureAssembler.transform(df)\n",
    "trainingData.show()\n",
    "# So we grouped the 2 columns into single Independent Feature which is our\n",
    "# input feature now. Our output feature is Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "839fdef3-15e9-4d31-a882-2c310a766fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData = trainingData.select(\"Independent Features\",\"Salary\")\n",
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fb8c801-51a9-4f9f-a7df-4093c00b9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "train_data, test_data = trainingData.randomSplit([0.75, 0.25]) # 75% - 25%\n",
    "\n",
    "regressor = LinearRegression(featuresCol=\"Independent Features\", labelCol=\"Salary\")\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08a8fe72-133d-431b-b94c-ae55022b22e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-64.8464, 1584.7554])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients # Will learn later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85c420ea-74be-4c0c-8d4b-81892c529b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15414.10693970376"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0511f0c4-2e5f-4b51-9f7a-d88d303fd154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------------+\n",
      "|Independent Features|Salary|       prediction|\n",
      "+--------------------+------+-----------------+\n",
      "|          [24.0,3.0]| 20000|18612.05915813422|\n",
      "+--------------------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results = regressor.evaluate(test_data)\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28c37b6c-e5e7-49b2-acdc-32965577ec45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1387.9408418657804, 1926379.7805190913)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError\n",
    "# Will Leaen in AAIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a0988-5700-4468-a4d1-970027d03078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
